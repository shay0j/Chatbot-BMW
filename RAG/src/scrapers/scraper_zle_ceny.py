from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from bs4 import BeautifulSoup
import json
import time
import re
from urllib.parse import urljoin, urlparse
from pathlib import Path
from datetime import datetime
import logging
from collections import deque

class CompleteBMWCrawler:
    """Kompletny crawler BMW zbierajƒÖcy WSZYSTKIE tre≈õci z naciskiem na modele"""
    
    def __init__(self):
        # Setup logging
        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
        self.logger = logging.getLogger(__name__)
        
        # STA≈ÅA ≈öCIE≈ªKA OUTPUTU
        self.output_dir = Path(r"C:\Users\hellb\Documents\Chatbot_BMW\RAG\output")
        
        # Stw√≥rz timestamp folder
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.output_dir = self.output_dir / f"bmw_complete_{timestamp}"
        
        self.logger.info(f"üìÅ Output directory: {self.output_dir}")
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Setup Chrome
        options = webdriver.ChromeOptions()
        options.add_argument('--disable-blink-features=AutomationControlled')
        options.add_experimental_option("excludeSwitches", ["enable-automation"])
        options.add_experimental_option('useAutomationExtension', False)
        options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--no-sandbox')
        options.add_argument('--window-size=1920,1080')
        
        # Dla szybszego dzia≈Çania - headless
        options.add_argument('--headless=new')
        
        self.driver = webdriver.Chrome(options=options)
        self.driver.set_page_load_timeout(30)
        
        # Stats and data
        self.visited_urls = set()
        self.to_visit = deque()
        self.scraped_data = []
        self.model_pages = []
        self.other_pages = []
        
        # Priorytetowe URL-e
        self.priority_urls = [
            "https://www.bmw.pl/pl/all-models.html",
            "https://www.bmw.pl/pl/all-models/x-models.html",
            "https://www.bmw.pl/pl/all-models/i-models.html",
            "https://www.bmw.pl/pl/all-models/m-models.html",
            "https://www.bmw.pl/pl/elektromobilnosc.html",
            "https://www.bmw.pl/pl/serwis-i-originalne-czesci.html",
            "https://www.bmw.pl/pl/konfigurator.html",
            "https://www.bmw.pl/pl/wiadomosci.html",
            "https://www.bmw.pl/pl/oferty.html",
        ]
        
        # Wzorce dla modeli - POPRAWIONE (bardziej restrykcyjne)
        self.model_patterns = {
            # Seria 1-8 - tylko pe≈Çne nazwy
            'Seria 1': r'\b1\s*Seria\b|\b1\s*Series\b',
            'Seria 2': r'\b2\s*Seria\b|\b2\s*Series\b',
            'Seria 3': r'\b3\s*Seria\b|\b3\s*Series\b',
            'Seria 4': r'\b4\s*Seria\b|\b4\s*Series\b',
            'Seria 5': r'\b5\s*Seria\b|\b5\s*Series\b',
            'Seria 7': r'\b7\s*Seria\b|\b7\s*Series\b',
            'Seria 8': r'\b8\s*Seria\b|\b8\s*Series\b',
            
            # X Series - tylko jako osobne s≈Çowa
            'X1': r'\bX1\b',
            'X2': r'\bX2\b',
            'X3': r'\bX3\b',
            'X4': r'\bX4\b',
            'X5': r'\bX5\b',
            'X6': r'\bX6\b',
            'X7': r'\bX7\b',
            
            # i Series (elektryczne)
            'i3': r'\bi3\b',
            'i4': r'\bi4\b',
            'iX': r'\biX\b',
            'iX3': r'\biX3\b',
            'iX1': r'\biX1\b',
            'i7': r'\bi7\b',
            'i5': r'\bi5\b',
            
            # M Series
            'M2': r'\bM2\b',
            'M3': r'\bM3\b',
            'M4': r'\bM4\b',
            'M5': r'\bM5\b',
            'M8': r'\bM8\b',
        }
        
        # Keywords dla r√≥≈ºnych kategorii
        self.category_keywords = {
            'models': ['model', 'silnik', 'moc', 'przyspieszenie', 'wymiary', 'cena', 'specyfikacja', 'dane techniczne'],
            'electric': ['elektryczny', '≈Çadowanie', 'bateria', 'zasiƒôg', 'ev', 'phev', 'kwh', 'wltp'],
            'service': ['serwis', 'przeglƒÖd', 'naprawa', 'czƒô≈õci', 'gwarancja', 'warsztat', 'oryginalne czƒô≈õci'],
            'configurator': ['konfigurator', 'wyposa≈ºenie', 'opcje', 'pakiety', 'kolory', 'tapicerka'],
            'offers': ['oferta', 'promocja', 'leasing', 'finansowanie', 'raty', 'rabat'],
            'news': ['nowo≈õƒá', 'premiera', 'aktualno≈õƒá', 'informacja', 'wydarzenie', 'wiadomo≈õci'],
        }
        
    def normalize_url(self, url, base_url=None):
        """Normalizuje URL"""
        if not url or url.startswith('javascript:') or url.startswith('mailto:'):
            return None
            
        # Usu≈Ñ fragment
        url = url.split('#')[0].strip()
        
        if not url:
            return None
            
        # Dodaj protok√≥≈Ç je≈õli brakuje
        if url.startswith('//'):
            url = 'https:' + url
        elif url.startswith('/'):
            if base_url:
                url = urljoin(base_url, url)
            else:
                url = 'https://www.bmw.pl' + url
        elif not url.startswith('http'):
            if base_url:
                url = urljoin(base_url, url)
            else:
                url = 'https://www.bmw.pl/' + url
        
        # Zawsze https
        if url.startswith('http://'):
            url = url.replace('http://', 'https://')
            
        # Usu≈Ñ parametry ≈õledzƒÖce
        url = re.sub(r'\?utm_.*', '', url)
        url = re.sub(r'\?gclid=.*', '', url)
        
        return url.strip('/')
    
    def is_valid_url(self, url):
        """Sprawdza czy URL jest poprawny do crawlowania"""
        if not url:
            return False
            
        # Musi byƒá BMW
        if 'bmw.pl' not in url:
            return False
            
        # Nie mo≈ºe byƒá ju≈º odwiedzony
        if url in self.visited_urls:
            return False
            
        # Unikaj niepotrzebnych rozszerze≈Ñ
        skip_extensions = ['.pdf', '.jpg', '.jpeg', '.png', '.gif', '.svg', 
                          '.mp4', '.avi', '.mov', '.mp3', '.wav', '.zip', 
                          '.rar', '.7z', '.exe', '.dmg', '.msi']
        
        if any(url.lower().endswith(ext) for ext in skip_extensions):
            return False
            
        # Unikaj social media i tracking
        skip_patterns = [
            r'facebook\.com', r'twitter\.com', r'instagram\.com', 
            r'linkedin\.com', r'youtube\.com', r'google\.com',
            r'/api/', r'/ajax/', r'/rest/', r'/graphql',
            r'/login', r'/register', r'/signin', r'/signup',
            r'/cart', r'/checkout', r'/basket', r'/order',
            r'\.xml$', r'\.json$', r'\.rss$',
        ]
        
        for pattern in skip_patterns:
            if re.search(pattern, url, re.IGNORECASE):
                return False
                
        return True
    
    def extract_model_from_url(self, url):
        """WyciƒÖga model z URL-a (najbardziej wiarygodne)"""
        url_lower = url.lower()
        
        # Mapowanie URL-i do modeli
        url_patterns = [
            (r'/x1[/-]', 'X1'),
            (r'/x2[/-]', 'X2'),
            (r'/x3[/-]', 'X3'),
            (r'/x4[/-]', 'X4'),
            (r'/x5[/-]', 'X5'),
            (r'/x6[/-]', 'X6'),
            (r'/x7[/-]', 'X7'),
            (r'/i3[/-]', 'i3'),
            (r'/i4[/-]', 'i4'),
            (r'/i5[/-]', 'i5'),
            (r'/i7[/-]', 'i7'),
            (r'/ix[/-]', 'iX'),
            (r'/m2[/-]', 'M2'),
            (r'/m3[/-]', 'M3'),
            (r'/m4[/-]', 'M4'),
            (r'/m5[/-]', 'M5'),
            (r'/m8[/-]', 'M8'),
            (r'/3-seria', 'Seria 3'),
            (r'/5-seria', 'Seria 5'),
            (r'/7-seria', 'Seria 7'),
            (r'/8-seria', 'Seria 8'),
            (r'/1-seria', 'Seria 1'),
            (r'/2-seria', 'Seria 2'),
            (r'/4-seria', 'Seria 4'),
            (r'/6-seria', 'Seria 6'),
            (r'/z4[/-]', 'Z4'),
        ]
        
        for pattern, model in url_patterns:
            if re.search(pattern, url_lower):
                return model
        
        return None
    
    def extract_content(self, soup, url):
        """WyciƒÖga tre≈õƒá ze strony"""
        try:
            # Tytu≈Ç
            title = soup.title.string if soup.title else ""
            
            # Meta description
            meta_desc = soup.find('meta', {'name': 'description'})
            description = meta_desc['content'] if meta_desc else ""
            
            # G≈Ç√≥wna tre≈õƒá - pr√≥buj r√≥≈ºnych selektor√≥w
            content_selectors = [
                'main', 
                'article', 
                '.content', 
                '.main-content',
                '#content', 
                '.article-content',
                '.text-content',
                '.body-content',
                '[role="main"]'
            ]
            
            main_content = None
            for selector in content_selectors:
                try:
                    main_content = soup.select_one(selector)
                    if main_content:
                        break
                except:
                    continue
            
            # Zbierz tekst
            text_parts = []
            
            if main_content:
                # Pobierz tekst z g≈Ç√≥wnego kontenera
                elements = main_content.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'li', 'dt', 'dd'])
                for elem in elements[:200]:  # Limit
                    text = elem.get_text(strip=True)
                    if 10 < len(text) < 2000:  # Minimum 10 znak√≥w, max 2000
                        text_parts.append(text)
            else:
                # Fallback: wszystkie paragrafy i nag≈Ç√≥wki
                elements = soup.find_all(['p', 'h1', 'h2', 'h3', 'h4'])
                for elem in elements[:100]:
                    text = elem.get_text(strip=True)
                    if 15 < len(text) < 1000:
                        text_parts.append(text)
            
            content = ' '.join(text_parts)
            word_count = len(content.split())
            
            # Je≈õli bardzo ma≈Ço tre≈õci, spr√≥buj inaczej
            if word_count < 30:
                # Spr√≥buj pobraƒá ca≈Çy tekst body
                body = soup.find('body')
                if body:
                    all_text = body.get_text()
                    # Podziel na zdania
                    sentences = re.split(r'[.!?]+', all_text)
                    content = ' '.join([s.strip() for s in sentences[:50] if len(s.strip()) > 20])
                    word_count = len(content.split())
            
            return {
                'title': title[:500],
                'description': description[:500],
                'content': content[:20000],  # Max 20k znak√≥w
                'word_count': word_count
            }
            
        except Exception as e:
            self.logger.error(f"B≈ÇƒÖd przy ekstrakcji tre≈õci: {e}")
            return {
                'title': '',
                'description': '',
                'content': '',
                'word_count': 0
            }
    
    def detect_models(self, text, url):
        """Wykrywa modele BMW w tek≈õcie - POPRAWIONE (mniej fa≈Çszywych pozytywnych)"""
        detected_models = []
        text_lower = text.lower()
        
        # Najpierw sprawd≈∫ URL (najbardziej wiarygodne)
        model_from_url = self.extract_model_from_url(url)
        if model_from_url:
            detected_models.append(model_from_url)
            return detected_models  # Je≈õli mamy model z URL, to wystarczy
        
        # Tylko je≈õli nie ma modelu z URL, sprawdzaj w tek≈õcie
        # Ale bardziej restrykcyjnie - szukaj pe≈Çnych nazw w kontek≈õcie
        model_context_patterns = {
            'Seria 1': r'(?:BMW\s+)?1\s*Seria|1\s*Series',
            'Seria 2': r'(?:BMW\s+)?2\s*Seria|2\s*Series',
            'Seria 3': r'(?:BMW\s+)?3\s*Seria|3\s*Series',
            'Seria 5': r'(?:BMW\s+)?5\s*Seria|5\s*Series',
            'Seria 7': r'(?:BMW\s+)?7\s*Seria|7\s*Series',
            'X1': r'(?:BMW\s+)?X1\b',
            'X3': r'(?:BMW\s+)?X3\b',
            'X5': r'(?:BMW\s+)?X5\b',
            'i4': r'(?:BMW\s+)?i4\b',
            'iX': r'(?:BMW\s+)?iX\b',
            'M3': r'(?:BMW\s+)?M3\b',
            'M5': r'(?:BMW\s+)?M5\b',
        }
        
        for model_name, pattern in model_context_patterns.items():
            if re.search(pattern, text, re.IGNORECASE):
                # Sprawd≈∫ czy to nie przypadkowe wystƒÖpienie
                # Szukaj w kontek≈õcie (w pobli≈ºu s≈Ç√≥w kluczowych)
                context_window = 50  # znak√≥w do przodu i ty≈Çu
                matches = list(re.finditer(pattern, text, re.IGNORECASE))
                
                for match in matches:
                    start = max(0, match.start() - context_window)
                    end = min(len(text), match.end() + context_window)
                    context = text[start:end].lower()
                    
                    # Sprawd≈∫ czy w kontek≈õcie sƒÖ s≈Çowa kluczowe BMW
                    bmw_keywords = ['bmw', 'model', 'silnik', 'moc', 'cena', 'specyfikacja']
                    if any(keyword in context for keyword in bmw_keywords):
                        if model_name not in detected_models:
                            detected_models.append(model_name)
                        break  # Jedno wystƒÖpienie wystarczy
        
        return list(set(detected_models))
    
    def categorize_content(self, text, url, detected_models):
        """Kategoryzuje tre≈õƒá"""
        categories = []
        text_lower = text.lower()
        url_lower = url.lower()
        
        # Priorytet: modele
        if detected_models:
            categories.append('models')
        
        # Sprawd≈∫ s≈Çowa kluczowe
        for category, keywords in self.category_keywords.items():
            if category == 'models' and detected_models:
                continue  # Ju≈º dodane
            
            keyword_count = sum(1 for keyword in keywords if keyword in text_lower)
            if keyword_count >= 2:
                categories.append(category)
        
        # Sprawd≈∫ URL
        url_categories = {
            'models': ['/models/', '/x', '/i', '/m', '/seria', '/series'],
            'electric': ['/elektro', '/electric', '/ev'],
            'service': ['/serwis', '/service', '/warsztat'],
            'configurator': ['/konfigurator', '/configure'],
            'news': ['/wiadomosci', '/news', '/aktualnosci'],
            'offers': ['/oferty', '/offers', '/promocje'],
        }
        
        for category, patterns in url_categories.items():
            if any(pattern in url_lower for pattern in patterns):
                if category not in categories:
                    categories.append(category)
        
        # Je≈õli ≈ºadna kategoria, to 'other'
        if not categories:
            categories.append('other')
        
        return categories
    
    def extract_specifications(self, soup, text, url):
        """WyciƒÖga specyfikacje techniczne - POPRAWIONE (lepsze parsowanie cen)"""
        specs = {
            'detected_models': [],
            'engine': {},
            'performance': {},
            'dimensions': {},
            'prices': [],
            'electric': {}
        }
        
        try:
            # Wykryj modele
            specs['detected_models'] = self.detect_models(text, url)
            
            # DODAJ TE LINIE: definicja text_lower
            text_lower = text.lower()
            
            # Moc silnika - POPRAWIONE (unikaj 0)
            power_patterns = [
                r'\b(\d{3,4})\s*(KM|kW|PS|hp)\b',  # 245 KM (min 3 cyfry)
                r'moc[:\s]+(\d{3,4})\s*(KM|kW)',   # moc: 245 KM
                r'(\d{3,4})\s*(?:koni|KM)\s*\(',   # 245 koni (
            ]
            
            for pattern in power_patterns:
                matches = re.findall(pattern, text, re.IGNORECASE)
                for value, unit in matches[:2]:  # Max 2 warto≈õci
                    try:
                        power_value = int(value)
                        if 80 <= power_value <= 800:  # Sensowny zakres mocy
                            if unit.upper() in ['KM', 'PS', 'HP']:
                                specs['engine']['power_hp'] = power_value
                            elif unit.upper() == 'KW':
                                specs['engine']['power_kw'] = power_value
                    except:
                        continue
            
            # Moment obrotowy
            torque_matches = re.findall(r'\b(\d{3,4})\s*Nm\b', text, re.IGNORECASE)
            for match in torque_matches[:1]:
                try:
                    torque = int(match)
                    if 200 <= torque <= 1000:  # Sensowny zakres
                        specs['engine']['torque_nm'] = torque
                except:
                    continue
            
            # Przyspieszenie 0-100
            accel_matches = re.findall(r'(\d[\d,.]{1,4})\s*s\s*(?:0[-\s]*100|0[-\s]*100\s*km/h)', text)
            for match in accel_matches[:1]:
                try:
                    accel = float(match.replace(',', '.'))
                    if 2.0 <= accel <= 10.0:  # Sensowny zakres
                        specs['performance']['acceleration_0_100'] = accel
                except:
                    continue
            
            # Prƒôdko≈õƒá maksymalna
            speed_matches = re.findall(r'\b(\d{2,3})\s*km/h\b', text)
            valid_speeds = []
            for match in speed_matches:
                try:
                    speed = int(match)
                    if 150 <= speed <= 350:  # Sensowny zakres
                        valid_speeds.append(speed)
                except:
                    continue
            
            if valid_speeds:
                specs['performance']['top_speed'] = max(valid_speeds)
            
            # Zasiƒôg (dla EV)
            range_matches = re.findall(r'\b(\d{3,4})\s*km\b', text)
            valid_ranges = []
            for match in range_matches:
                try:
                    range_val = int(match)
                    if 200 <= range_val <= 800:  # Sensowny zakres dla EV
                        valid_ranges.append(range_val)
                except:
                    continue
            
            if valid_ranges:
                specs['electric']['range_km'] = max(valid_ranges)
            
            # Ceny - ZNACZNIE POPRAWIONE (konkretne selektory HTML)
            # Pr√≥buj r√≥≈ºnych podej≈õƒá do znalezienia cen
            price_texts = []
            
            # 1. Szukaj w specjalnych elementach HTML
            price_selectors = [
                '.price', '.Price', '.offer-price', '.product-price',
                '[data-price]', '[data-testid="price"]', '.cena',
                '.vehicle-price', '.car-price', '.model-price'
            ]
            
            for selector in price_selectors:
                price_elements = soup.select(selector)
                for elem in price_elements[:5]:
                    price_text = elem.get_text(strip=True)
                    if price_text and len(price_text) > 3:
                        price_texts.append(price_text)
            
            # 2. Szukaj w tekstowych wzorcach (bardziej restrykcyjne)
            price_patterns = [
                # Wzorzec: od 100 000 z≈Ç
                r'od\s+(\d{1,3}(?:\s?\d{3}){1,2}[,\d]*)\s*z[≈Çl]\b',
                # Wzorzec: cena od 100 000 z≈Ç
                r'cena\s+od\s+(\d{1,3}(?:\s?\d{3}){1,2}[,\d]*)\s*z[≈Çl]\b',
                # Wzorzec: 100 000 z≈Ç
                r'\b(\d{1,3}(?:\s?\d{3}){1,2}[,\d]*)\s*z[≈Çl]\b(?!\s*(?:od|do|za))',
                # Wzorzec: 100.000 z≈Ç
                r'\b(\d{1,3}[.,]\d{3}(?:[.,]\d{3})?)\s*z[≈Çl]\b',
                # Wzorzec z EUR
                r'\b(\d{1,3}[.,]\d{3}(?:[.,]\d{3})?)\s*‚Ç¨\b',
            ]
            
            for pattern in price_patterns:
                matches = re.findall(pattern, text, re.IGNORECASE)
                for match in matches[:3]:  # Max 3 r√≥≈ºne ceny z tego wzorca
                    if isinstance(match, tuple):
                        match = match[0]  # Je≈õli wzorzec ma grupy
                    price_texts.append(f"{match} z≈Ç")
            
            # 3. Przetw√≥rz znalezione ceny
            seen_prices = set()
            for price_text in price_texts:
                try:
                    # Oczy≈õƒá tekst ceny
                    clean_text = re.sub(r'[^\d,\s.]', '', price_text)
                    clean_text = clean_text.replace(',', '.').strip()
                    
                    # Usu≈Ñ spacje z liczb
                    clean_text = re.sub(r'\s+', '', clean_text)
                    
                    # Konwertuj na liczbƒô
                    if '.' in clean_text:
                        # Sprawd≈∫ czy to dziesiƒôtne czy separator tysiƒôcy
                        parts = clean_text.split('.')
                        if len(parts) == 2 and len(parts[1]) <= 2:
                            # Liczba dziesiƒôtna (z groszami)
                            price_value = float(clean_text)
                        else:
                            # Separator tysiƒôcy (np. 100.000)
                            clean_text = clean_text.replace('.', '')
                            price_value = float(clean_text)
                    else:
                        price_value = float(clean_text)
                    
                    # Sprawd≈∫ czy to sensowna cena samochodu BMW
                    if 30000 <= price_value <= 2000000:
                        # ZaokrƒÖglij dla unikniƒôcia ma≈Çych r√≥≈ºnic
                        rounded_price = round(price_value, -2)  # do setek
                        if rounded_price not in seen_prices:
                            seen_prices.add(rounded_price)
                            specs['prices'].append({
                                'amount': price_value,
                                'currency': 'PLN',
                                'formatted': f"{price_value:,.0f} z≈Ç".replace(',', ' '),
                                'source_text': price_text[:50]
                            })
                    
                except Exception as e:
                    continue
            
            # Ogranicz do 5 cen
            if specs['prices']:
                specs['prices'] = specs['prices'][:5]
            
            # Pojemno≈õƒá silnika
            capacity_match = re.search(r'\b(\d{1,3}[,\d]*)\s*cm[¬≥3]\b', text)
            if capacity_match:
                specs['engine']['capacity_cc'] = capacity_match.group(1)
            
            # Pojemno≈õƒá baterii
            battery_match = re.search(r'\b(\d{2,3}[,\d]*)\s*kWh\b', text, re.IGNORECASE)
            if battery_match:
                specs['electric']['battery_kwh'] = battery_match.group(1)
            
            # ≈Åadowanie
            charging_match = re.search(r'\b(\d{1,3}[,\d]*)\s*kW\s*(?:AC|DC)?\s*≈Çadowan', text, re.IGNORECASE)
            if charging_match:
                specs['electric']['charging_kw'] = charging_match.group(1)
            
            # Sprawd≈∫ czy to strona specyfikacji
            specs_page_keywords = ['dane techniczne', 'technical data', 'specyfikacja', 'technische daten']
            for keyword in specs_page_keywords:
                if keyword in text_lower:
                    specs['is_specs_page'] = True
                    break
            
            # Oczy≈õƒá puste warto≈õci z engine
            if 'engine' in specs:
                specs['engine'] = {k: v for k, v in specs['engine'].items() if v not in [0, None, '']}
            
        except Exception as e:
            self.logger.warning(f"B≈ÇƒÖd przy ekstrakcji specyfikacji: {e}")
        
        return specs
    
    def scrape_page(self, url):
        """Scrapuje pojedynczƒÖ stronƒô"""
        if url in self.visited_urls:
            return None
            
        self.logger.info(f"üåê {url[:80]}...")
        self.visited_urls.add(url)
        
        try:
            # Otw√≥rz stronƒô
            self.driver.get(url)
            time.sleep(2)  # Czekaj na za≈Çadowanie
            
            # Sprawd≈∫ czy to nie strona b≈Çƒôdu
            page_title = self.driver.title
            if any(error_word in page_title.lower() for error_word in 
                  ['error', 'b≈ÇƒÖd', 'nie znaleziono', 'not found', '404']):
                self.logger.warning(f"  ‚ö†Ô∏è Strona b≈Çƒôdu")
                return None
            
            # Pobierz HTML
            html = self.driver.page_source
            soup = BeautifulSoup(html, 'html.parser')
            
            # WyciƒÖgnij tre≈õƒá
            content_data = self.extract_content(soup, url)
            
            # Je≈õli za ma≈Ço tre≈õci, pomi≈Ñ
            if content_data['word_count'] < 30:
                self.logger.info(f"  ‚è≠Ô∏è Za ma≈Ço tre≈õci ({content_data['word_count']} s≈Ç√≥w)")
                return None
            
            # WyciƒÖgnij specyfikacje - TERAZ PRZEKAZUJEMY SOUP
            full_text = content_data['content'] + ' ' + content_data['title'] + ' ' + content_data['description']
            specs = self.extract_specifications(soup, full_text, url)
            
            # Wykryj modele
            detected_models = self.detect_models(full_text, url)
            
            # Kategoryzuj
            categories = self.categorize_content(full_text, url, detected_models)
            
            # Przygotuj dane
            page_data = {
                'url': url,
                'title': content_data['title'],
                'description': content_data['description'],
                'content': content_data['content'],
                'word_count': content_data['word_count'],
                'categories': categories,
                'detected_models': detected_models,
                'specifications': specs,
                'is_model_page': 'models' in categories or bool(detected_models),
                'model_from_url': self.extract_model_from_url(url),
                'scraped_at': datetime.now().isoformat(),
                'priority': 1 if ('models' in categories or bool(detected_models)) else 2
            }
            
            # Dodaj do odpowiedniej listy
            if page_data['is_model_page']:
                self.model_pages.append(page_data)
                
                # Log z lepszymi informacjami
                model_info = []
                if page_data['model_from_url']:
                    model_info.append(f"URL: {page_data['model_from_url']}")
                if detected_models:
                    model_info.append(f"Tekst: {', '.join(detected_models[:2])}")
                
                spec_info = []
                if specs.get('engine', {}).get('power_hp'):
                    spec_info.append(f"{specs['engine']['power_hp']}KM")
                if specs.get('prices'):
                    price = specs['prices'][0]['formatted']
                    spec_info.append(f"{price}")
                
                log_msg = f"  üöó"
                if model_info:
                    log_msg += f" {' | '.join(model_info)}"
                if spec_info:
                    log_msg += f" [{' | '.join(spec_info)}]"
                log_msg += f" ({content_data['word_count']} s≈Ç√≥w, {len(specs.get('prices', []))} cen)"
                
                self.logger.info(log_msg)
            else:
                self.other_pages.append(page_data)
                self.logger.info(f"  üìÑ Inne [{categories[0] if categories else 'other'}] - {content_data['word_count']} s≈Ç√≥w")
            
            self.scraped_data.append(page_data)
            
            # Znajd≈∫ linki na stronie
            self.extract_links(url, soup)
            
            return page_data
            
        except Exception as e:
            self.logger.error(f"  ‚ùå B≈ÇƒÖd: {str(e)[:80]}")
            return None
    
    def extract_links(self, base_url, soup):
        """WyciƒÖga linki ze strony"""
        try:
            links_found = 0
            
            for link in soup.find_all('a', href=True)[:80]:  # Limit 80 link√≥w
                try:
                    href = link['href']
                    
                    # Normalizuj URL
                    normalized = self.normalize_url(href, base_url)
                    
                    if normalized and self.is_valid_url(normalized):
                        # Sprawd≈∫ czy ju≈º w kolejce
                        if normalized not in self.to_visit and normalized not in self.visited_urls:
                            
                            # Okre≈õl priorytet
                            priority = self.get_url_priority(normalized)
                            
                            # Dodaj do kolejki z priorytetem
                            self.to_visit.append((priority, normalized))
                            links_found += 1
                            
                except Exception as e:
                    continue
            
            # Posortuj kolejkƒô wed≈Çug priorytetu
            self.to_visit = deque(sorted(self.to_visit, key=lambda x: x[0]))
            
            if links_found > 0:
                self.logger.debug(f"Znaleziono {links_found} nowych link√≥w")
                
        except Exception as e:
            self.logger.error(f"B≈ÇƒÖd przy ekstrakcji link√≥w: {e}")
    
    def get_url_priority(self, url):
        """Okre≈õla priorytet URL-a"""
        url_lower = url.lower()
        
        # Priorytet 1: Strony z modelami i specyfikacjami
        if any(pattern in url_lower for pattern in 
              ['/models/', '/x', '/i', '/m', '/seria', '/series',
               'dane-techniczne', 'technical-data', 'specyfikacja']):
            return 1
        
        # Priorytet 2: Wa≈ºne sekcje
        if any(pattern in url_lower for pattern in 
              ['/elektro', '/electric', '/konfigurator', '/service', '/serwis']):
            return 2
        
        # Priorytet 3: Inne tre≈õci
        return 3
    
    def crawl(self):
        """G≈Ç√≥wna funkcja crawlowania"""
        self.logger.info("=" * 70)
        self.logger.info("üöó POPRAWIONY BMW CRAWLER - START")
        self.logger.info("=" * 70)
        
        start_time = time.time()
        
        try:
            # 1. Zacznij od priorytetowych URL-i
            self.logger.info("\nüéØ Rozpoczynam od priorytetowych stron...")
            for url in self.priority_urls:
                if self.is_valid_url(url):
                    self.to_visit.append((1, url))
            
            # 2. G≈Ç√≥wna pƒôtla crawlowania
            max_pages = 80  # Maksymalna liczba stron
            pages_scraped = 0
            
            while self.to_visit and pages_scraped < max_pages:
                try:
                    # Pobierz nastƒôpny URL (z najwy≈ºszym priorytetem)
                    if not self.to_visit:
                        break
                    
                    priority, url = self.to_visit.popleft()
                    
                    # Scrapuj stronƒô
                    result = self.scrape_page(url)
                    if result:
                        pages_scraped += 1
                        
                        # Co 10 stron poka≈º status
                        if pages_scraped % 10 == 0:
                            elapsed = time.time() - start_time
                            self.logger.info(f"\nüìä Status: {pages_scraped}/{max_pages} stron, {elapsed:.0f}s")
                            self.logger.info(f"   üöó Modele: {len(self.model_pages)}")
                            self.logger.info(f"   üì∞ Inne: {len(self.other_pages)}")
                            # Statystyki cen
                            all_prices = []
                            for page in self.model_pages:
                                prices = page.get('specifications', {}).get('prices', [])
                                all_prices.extend(prices)
                            self.logger.info(f"   üí∞ Unikalne ceny: {len(set(p['amount'] for p in all_prices if 'amount' in p))}")
                    
                    # Rate limiting
                    time.sleep(1.2 if priority == 1 else 0.8)
                    
                except Exception as e:
                    self.logger.error(f"B≈ÇƒÖd w g≈Ç√≥wnej pƒôtli: {e}")
                    continue
            
            self.logger.info(f"\n‚úÖ Zako≈Ñczono crawlowanie. Zebrano {pages_scraped} stron.")
            
        except KeyboardInterrupt:
            self.logger.info("\nüõë Crawler zatrzymany przez u≈ºytkownika")
        except Exception as e:
            self.logger.error(f"\n‚ùå Nieoczekiwany b≈ÇƒÖd: {e}")
            import traceback
            traceback.print_exc()
        finally:
            # Zawsze zapisz dane
            self.save_all_data()
            self.print_summary(start_time)
            
            # Zamknij przeglƒÖdarkƒô
            self.driver.quit()
    
    def save_progress(self):
        """Zapisuje postƒôp tymczasowo"""
        try:
            # Zapisz tymczasowe dane
            temp_file = self.output_dir / "temp_progress.json"
            with open(temp_file, 'w', encoding='utf-8') as f:
                json.dump({
                    'visited_urls': list(self.visited_urls),
                    'scraped_count': len(self.scraped_data),
                    'timestamp': datetime.now().isoformat()
                }, f, ensure_ascii=False, indent=2)
                
            self.logger.debug("Zapisano postƒôp tymczasowo")
        except Exception as e:
            self.logger.error(f"B≈ÇƒÖd przy zapisie postƒôpu: {e}")
    
    def save_all_data(self):
        """Zapisuje wszystkie zebrane dane"""
        if not self.scraped_data:
            self.logger.warning("Brak danych do zapisania!")
            return
        
        try:
            # 1. Wszystkie dane (JSONL dla ≈Çatwego przetwarzania)
            all_file = self.output_dir / "all_data.jsonl"
            with open(all_file, 'w', encoding='utf-8') as f:
                for item in self.scraped_data:
                    json.dump(item, f, ensure_ascii=False)
                    f.write('\n')
            
            # 2. Tylko modele (JSON)
            models_file = self.output_dir / "models_data.json"
            with open(models_file, 'w', encoding='utf-8') as f:
                json.dump(self.model_pages, f, ensure_ascii=False, indent=2)
            
            # 3. Inne tre≈õci (JSON)
            other_file = self.output_dir / "other_data.json"
            with open(other_file, 'w', encoding='utf-8') as f:
                json.dump(self.other_pages, f, ensure_ascii=False, indent=2)
            
            # 4. Statystyki
            stats = self.calculate_statistics()
            stats_file = self.output_dir / "stats.json"
            with open(stats_file, 'w', encoding='utf-8') as f:
                json.dump(stats, f, ensure_ascii=False, indent=2)
            
            # 5. URL-e (dla debugowania)
            urls_file = self.output_dir / "visited_urls.txt"
            with open(urls_file, 'w', encoding='utf-8') as f:
                for url in sorted(self.visited_urls):
                    f.write(url + '\n')
            
            # 6. Specjalny plik z cenami (dla analizy)
            prices_file = self.output_dir / "prices_analysis.json"
            all_prices_data = []
            for page in self.scraped_data:
                prices = page.get('specifications', {}).get('prices', [])
                if prices:
                    for price in prices:
                        all_prices_data.append({
                            'url': page['url'],
                            'title': page['title'][:100],
                            'model': page.get('model_from_url', ''),
                            'detected_models': page.get('detected_models', []),
                            'price': price
                        })
            
            if all_prices_data:
                with open(prices_file, 'w', encoding='utf-8') as f:
                    json.dump(all_prices_data, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"\nüíæ Dane zapisane w: {self.output_dir}")
            self.logger.info(f"   üìÑ Wszystkie dane: {all_file}")
            self.logger.info(f"   üöó Modele: {models_file}")
            self.logger.info(f"   üì∞ Inne: {other_file}")
            self.logger.info(f"   üìä Statystyki: {stats_file}")
            if all_prices_data:
                self.logger.info(f"   üí∞ Analiza cen: {prices_file}")
            
        except Exception as e:
            self.logger.error(f"B≈ÇƒÖd przy zapisie danych: {e}")
    
    def calculate_statistics(self):
        """Oblicza statystyki"""
        stats = {
            'total_pages': len(self.scraped_data),
            'model_pages': len(self.model_pages),
            'other_pages': len(self.other_pages),
            'visited_urls': len(self.visited_urls),
            'total_words': sum(p['word_count'] for p in self.scraped_data),
            'avg_words_per_page': sum(p['word_count'] for p in self.scraped_data) / len(self.scraped_data) if self.scraped_data else 0,
            'categories': {},
            'models_found': [],
            'models_from_url': [],
            'specs_stats': {},
            'prices_stats': {},
            'output_dir': str(self.output_dir),
            'scraped_at': datetime.now().isoformat()
        }
        
        # Licz kategorie
        for page in self.scraped_data:
            for category in page.get('categories', []):
                stats['categories'][category] = stats['categories'].get(category, 0) + 1
        
        # Zebrane modele z tekstu
        all_models = []
        for page in self.model_pages:
            all_models.extend(page.get('detected_models', []))
        
        if all_models:
            from collections import Counter
            model_counts = Counter(all_models)
            stats['models_found'] = dict(model_counts.most_common(20))
        
        # Modele z URL (najbardziej wiarygodne)
        url_models = []
        for page in self.model_pages:
            if page.get('model_from_url'):
                url_models.append(page['model_from_url'])
        
        if url_models:
            from collections import Counter
            url_model_counts = Counter(url_models)
            stats['models_from_url'] = dict(url_model_counts.most_common(20))
        
        # Statystyki specyfikacji
        specs_fields = ['power_hp', 'power_kw', 'torque_nm', 'acceleration_0_100', 'top_speed']
        for field in specs_fields:
            values = []
            for page in self.scraped_data:
                specs = page.get('specifications', {})
                if field == 'power_hp' and 'engine' in specs:
                    values.append(specs['engine'].get(field))
                elif field == 'power_kw' and 'engine' in specs:
                    values.append(specs['engine'].get(field))
                elif field == 'torque_nm' and 'engine' in specs:
                    values.append(specs['engine'].get(field))
                elif field in ['acceleration_0_100', 'top_speed'] and 'performance' in specs:
                    values.append(specs['performance'].get(field))
            
            # Oczy≈õƒá None warto≈õci
            values = [v for v in values if v is not None]
            if values:
                stats['specs_stats'][field] = {
                    'count': len(values),
                    'avg': sum(values) / len(values),
                    'min': min(values),
                    'max': max(values)
                }
        
        # STATYSTYKI CEN - POPRAWIONE
        all_prices = []
        price_sources = {}
        for page in self.scraped_data:
            prices = page.get('specifications', {}).get('prices', [])
            for price in prices:
                if 'amount' in price:
                    all_prices.append(price['amount'])
                    
                    # Zlicz ≈∫r√≥d≈Ça cen
                    url_key = page['url']
                    price_sources[url_key] = price_sources.get(url_key, 0) + 1
        
        if all_prices:
            # Unikalne ceny (zaokrƒÖglone do tysiƒÖca)
            unique_prices = set(round(price / 1000) * 1000 for price in all_prices)
            
            stats['prices_stats'] = {
                'total_found': len(all_prices),
                'unique_prices': len(unique_prices),
                'pages_with_prices': sum(1 for page in self.scraped_data if page.get('specifications', {}).get('prices')),
                'min': min(all_prices),
                'max': max(all_prices),
                'avg': sum(all_prices) / len(all_prices),
                'avg_formatted': f"{sum(all_prices) / len(all_prices):,.0f} z≈Ç".replace(',', ' '),
                'price_distribution': {
                    '< 100k': len([p for p in all_prices if p < 100000]),
                    '100k-200k': len([p for p in all_prices if 100000 <= p < 200000]),
                    '200k-300k': len([p for p in all_prices if 200000 <= p < 300000]),
                    '300k-500k': len([p for p in all_prices if 300000 <= p < 500000]),
                    '500k+': len([p for p in all_prices if p >= 500000]),
                },
                'sources_count': len(price_sources)
            }
        
        return stats
    
    def log_status(self, pages_scraped, start_time):
        """Loguje status crawlera"""
        elapsed = time.time() - start_time
        elapsed_min = elapsed / 60
        
        self.logger.info("\n" + "=" * 50)
        self.logger.info(f"üìä STATUS: Strona {pages_scraped}")
        self.logger.info(f"‚è±Ô∏è Czas: {elapsed_min:.1f} minut")
        self.logger.info(f"üöó Stron z modelami: {len(self.model_pages)}")
        self.logger.info(f"üì∞ Innych stron: {len(self.other_pages)}")
        self.logger.info(f"üîó W kolejce: {len(self.to_visit)}")
        self.logger.info("=" * 50)
    
    def print_summary(self, start_time):
        """Wy≈õwietla podsumowanie"""
        elapsed = time.time() - start_time
        elapsed_min = elapsed / 60
        
        stats = self.calculate_statistics()
        
        print("\n" + "=" * 70)
        print("üéâ POPRAWIONY BMW CRAWLER - PODSUMOWANIE")
        print("=" * 70)
        
        print(f"\nüìä STATYSTYKI:")
        print(f"  Stron zebranych: {stats['total_pages']}")
        print(f"  Stron z modelami: {stats['model_pages']}")
        print(f"  Innych stron: {stats['other_pages']}")
        print(f"  S≈Çowa og√≥≈Çem: {stats['total_words']:,}")
        print(f"  ≈örednio s≈Ç√≥w/stronƒô: {stats['avg_words_per_page']:.0f}")
        print(f"  ‚è±Ô∏è Czas pracy: {elapsed_min:.1f} minut")
        
        if elapsed > 0:
            pages_per_min = stats['total_pages'] / elapsed_min
            print(f"  üìà Wydajno≈õƒá: {pages_per_min:.1f} stron/minutƒô")
        
        print(f"\nüìÇ KATEGORIE:")
        for category, count in sorted(stats['categories'].items(), key=lambda x: x[1], reverse=True):
            print(f"  {category}: {count} stron")
        
        if stats.get('models_from_url'):
            print(f"\nüöó MODELE (z URL - NAJWIARYGODNIEJSZE):")
            for model, count in list(stats['models_from_url'].items())[:15]:
                print(f"  {model}: {count} stron")
        elif stats.get('models_found'):
            print(f"\nüöó WYKRYTE MODELE (z tekstu):")
            for model, count in list(stats['models_found'].items())[:15]:
                print(f"  {model}: {count} stron")
        
        if stats.get('specs_stats'):
            print(f"\nüîß SPECYFIKACJE:")
            if 'power_hp' in stats['specs_stats']:
                power = stats['specs_stats']['power_hp']
                print(f"  ≈örednia moc: {power['avg']:.0f} KM ({power['count']} stron)")
            if 'acceleration_0_100' in stats['specs_stats']:
                accel = stats['specs_stats']['acceleration_0_100']
                print(f"  0-100 km/h: {accel['avg']:.1f} s ({accel['count']} stron)")
        
        if stats.get('prices_stats'):
            prices = stats['prices_stats']
            print(f"\nüí∞ ANALIZA CEN:")
            print(f"  Znalezionych cen: {prices['total_found']}")
            print(f"  Unikalnych cen: {prices['unique_prices']}")
            print(f"  Stron z cenami: {prices['pages_with_prices']}")
            print(f"  ≈örednia cena: {prices['avg_formatted']}")
            print(f"  Zakres: {prices['min']:,.0f} - {prices['max']:,.0f} z≈Ç")
            
            if 'price_distribution' in prices:
                print(f"  Rozk≈Çad cen:")
                for range_name, count in prices['price_distribution'].items():
                    if count > 0:
                        print(f"    {range_name}: {count} cen")
        
        print(f"\nüíæ DANE ZAPISANE W: {self.output_dir}")
        print("=" * 70)

# Uruchom
if __name__ == "__main__":
    crawler = CompleteBMWCrawler()
    crawler.crawl()